{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import os.path as path\n",
    "from os import listdir \n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def your own path\n",
    "ROOT = \"/Users/hannabritt/Documents/IntrinsicDimDeep\"\n",
    "\n",
    "os.chdir(ROOT)\n",
    "import sys\n",
    "sys.path.insert(0, ROOT)\n",
    "\n",
    "# and here\n",
    "results_folder = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N.of classes : 7\n"
     ]
    }
   ],
   "source": [
    "#Select classes\n",
    "category_tags = ['n01882714','n02086240','n02087394','n02094433','n02100583','n02100735','n02279972', 'mix']\n",
    "n_objects = len(category_tags) - 1\n",
    "print('N.of classes : {}'.format(n_objects))\n",
    "\n",
    "# random generator init\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(999)\n",
    "\n",
    "# parameters\n",
    "arch = 'vgg16'\n",
    "nsamples = 500\n",
    "bs = 16\n",
    "\n",
    "# functions to select checkpoint layers and to determine their depths\n",
    "# (this works for AlexNet and VGG-like architectures)\n",
    "def getDepths(model):    \n",
    "    count = 0    \n",
    "    modules = []\n",
    "    names = []\n",
    "    depths = []    \n",
    "    modules.append('input')\n",
    "    names.append('input')\n",
    "    depths.append(0)    \n",
    "    \n",
    "    for i,module in enumerate(model.features):       \n",
    "        name = module.__class__.__name__\n",
    "        if 'Conv2d' in name or 'Linear' in name:\n",
    "            count += 1\n",
    "        if 'MaxPool2d' in name:\n",
    "            modules.append(module)\n",
    "            depths.append(count)\n",
    "            names.append('MaxPool2d')            \n",
    "    for i,module in enumerate(model.classifier):\n",
    "        name = module.__class__.__name__\n",
    "        if 'Linear' in name:\n",
    "            modules.append(module)    \n",
    "            count += 1\n",
    "            depths.append(count + 1)\n",
    "            names.append('Linear')                       \n",
    "    depths = np.array(depths)   \n",
    "    return modules, names, depths\n",
    "\n",
    "def getLayerDepth(layer):\n",
    "    count = 0\n",
    "    for m in layer:\n",
    "        for c in m.children():\n",
    "            name = c.__class__.__name__\n",
    "            if 'Conv' in name:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# functions to select checkpoint layers and to determine their depths\n",
    "# (this works for ResNets architectures)\n",
    "def getResNetsDepths(model):    \n",
    "    modules = []\n",
    "    names = []\n",
    "    depths = []  \n",
    "    \n",
    "    # input\n",
    "    count = 0\n",
    "    modules.append('input')\n",
    "    names.append('input')\n",
    "    depths.append(count)           \n",
    "    # maxpooling\n",
    "    count += 1\n",
    "    modules.append(model.maxpool)\n",
    "    names.append('maxpool')\n",
    "    depths.append(count)     \n",
    "    # 1 \n",
    "    count += getLayerDepth(model.layer1)\n",
    "    modules.append(model.layer1)\n",
    "    names.append('layer1')\n",
    "    depths.append(count)         \n",
    "    # 2\n",
    "    count += getLayerDepth(model.layer2)\n",
    "    modules.append(model.layer2)\n",
    "    names.append('layer2')\n",
    "    depths.append(count)      \n",
    "    # 3\n",
    "    count += getLayerDepth(model.layer3)\n",
    "    modules.append(model.layer3)\n",
    "    names.append('layer3')\n",
    "    depths.append(count)     \n",
    "    # 4 \n",
    "    count += getLayerDepth(model.layer4)\n",
    "    modules.append(model.layer4)\n",
    "    names.append('layer4')\n",
    "    depths.append(count)      \n",
    "    # average pooling\n",
    "    count += 1\n",
    "    modules.append(model.avgpool)\n",
    "    names.append('avgpool')\n",
    "    depths.append(count)     \n",
    "    # output\n",
    "    count += 1\n",
    "    modules.append(model.fc)\n",
    "    names.append('fc')\n",
    "    depths.append(count)                      \n",
    "    depths = np.array(depths)    \n",
    "    return modules, names, depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode : False\n",
      "List of layers from which to extract representations: ['input', 'MaxPool2d', 'MaxPool2d', 'MaxPool2d', 'MaxPool2d', 'MaxPool2d', 'Linear', 'Linear', 'Linear']\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "model = vgg16(pretrained=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# this switch to evaluation mode your network: in this way dropout and batchnorm \n",
    "# no more active and you can use the network as a 'passive' feedforward device; \n",
    "# forgetting this produces catastrophically wrong results (I know because I did it)\n",
    "model.eval()\n",
    "print('Training mode : {}'.format(model.training))\n",
    "\n",
    "modules, names, depths = getDepths(model)\n",
    "print('List of layers from which to extract representations: {}'.format(names))\n",
    "\n",
    "\n",
    "mean_imgs = [0.485, 0.456, 0.406]\n",
    "std_imgs = [0.229, 0.224, 0.225]\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Data transformations (same as suggested by Soumith Chintala's script)\n",
    "data_transforms =  transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Extraction of representations\n",
    "This will extract for each class the representations of $\\sim 500$ images from all checkpoints, including input and output.\n",
    "\n",
    "It will save these representations as matrices of shape (n.images,embedding dimension).\n",
    "\n",
    "A typical filename will be n02086240_5.npy which means that this file contains the representations of class n02086240 extracted at the sixt (5+1) checkpoint layer, which is the max pooling after the last convolutional layer, as you can easily check by printing the list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input\n",
      "1 MaxPool2d\n",
      "2 MaxPool2d\n",
      "3 MaxPool2d\n",
      "4 MaxPool2d\n",
      "5 MaxPool2d\n",
      "6 Linear\n",
      "7 Linear\n",
      "8 Linear\n",
      "Processing class: n01882714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,name in enumerate(names):\n",
    "    print(i,name)\n",
    "    \n",
    "n_layers = len(modules)\n",
    "embdims=[] # store the embedding dimension of the checkpoint layers (n.of units)\n",
    "\n",
    "\n",
    "for i,tag in enumerate(category_tags):\n",
    "    \n",
    "    print('Processing class: {}'.format(tag))\n",
    "    data_folder = join(ROOT, 'data', 'imagenet_training_single_objs', tag)\n",
    "    image_dataset = datasets.ImageFolder(join(data_folder), data_transforms)           \n",
    "    dataloader = torch.utils.data.DataLoader(image_dataset, \n",
    "                                             batch_size=bs, \n",
    "                                             shuffle=True, \n",
    "                                             num_workers=1)  \n",
    "\n",
    "    for l,module in enumerate(modules):    \n",
    "        for k, data in enumerate(dataloader, 0):\n",
    "            if k*bs > nsamples:\n",
    "                break\n",
    "            else:  \n",
    "                inputs, _ = data                          \n",
    "                if module == 'input':                \n",
    "                    hout = inputs                      \n",
    "                else:            \n",
    "                    hout = []\n",
    "                    def hook(module, input, output):\n",
    "                        hout.append(output)                \n",
    "                    handle = module.register_forward_hook(hook)                            \n",
    "                    out = model(inputs.to(device))\n",
    "                    del out   \n",
    "                    \n",
    "                    hout = hout[0] \n",
    "                            \n",
    "                    handle.remove()\n",
    "\n",
    "                if k == 0:\n",
    "                    Out = hout.view(inputs.shape[0], -1).cpu().data    \n",
    "                else :               \n",
    "                    Out = torch.cat((Out, hout.view(inputs.shape[0], -1).cpu().data),0) \n",
    "                hout = hout.detach().cpu()\n",
    "                del hout\n",
    "\n",
    "        Out = Out.detach().cpu()  \n",
    "        embdims.append(Out.shape[1])\n",
    "        \n",
    "        \n",
    "        np.save(join(results_folder, tag + '_' + str(l) + '.npy' ), Out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
